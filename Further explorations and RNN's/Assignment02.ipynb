{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Assignment 02: Neural Embedding and Sequence Modelling</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Set random seeds</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing tensorflow and numpy and setting random seeds for TF and numpy. You can use any seeds you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(123456)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Download and preprocess the data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the train set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR), \n",
    "- entity (ENTY), \n",
    "- description (DESC), \n",
    "- human (HUM), \n",
    "- location (LOC) and \n",
    "- numeric (NUM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data is an inital and important step in any machine learning or deep learning projects. The following *DataManager* class helps you to download data and preprocess data for the later steps of a deep learning project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, verbose=True, maxlen= 50, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = maxlen\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "        \n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose= True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_names):\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            self.str_questions= list(); self.str_labels= list()\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str= row.split(\":\")\n",
    "                    label, question= row_str[0], row_str[1]\n",
    "                    question= question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len= len(self.str_questions[-1])\n",
    "         \n",
    "        # turns labels into numbers\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def manipulate_data(self):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.str_questions)\n",
    "        self.numeral_data = tokenizer.texts_to_sequences(self.str_questions)\n",
    "        self.numeral_data = tf.keras.preprocessing.sequence.pad_sequences(self.numeral_data, padding='post', truncating= 'post', maxlen= self.maxlen)\n",
    "        self.word2idx = tokenizer.word_index\n",
    "        self.word2idx = {k:v for k,v in self.word2idx.items()}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "    \n",
    "    def train_valid_split(self, train_ratio=0.9):\n",
    "        idxs = np.random.permutation(np.arange(len(self.str_questions)))\n",
    "        train_size = int(train_ratio*len(idxs)) +1\n",
    "        self.train_str_questions, self.valid_str_questions = self.str_questions[0:train_size], self.str_questions[train_size:]\n",
    "        self.train_numeral_data, self.valid_numeral_data = self.numeral_data[0:train_size], self.numeral_data[train_size:]\n",
    "        self.train_numeral_labels, self.valid_numeral_labels = self.numeral_labels[0:train_size], self.numeral_labels[train_size:]\n",
    "        self.tf_train_set = tf.data.Dataset.from_tensor_slices((self.train_numeral_data, self.train_numeral_labels))\n",
    "        self.tf_valid_set = tf.data.Dataset.from_tensor_slices((self.valid_numeral_data, self.valid_numeral_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Sample questions... \n",
      "\n",
      "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data(\"Data/\", [\"train_set.label\"])   # read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.1**</span> \n",
    "**What is the purpose of `self.train_str_questions` and `self.train_numeral_labels`? Write your code to print out the first five questions with labels in the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "self.train_str_questions is a list that is used to store all the questions in the training set and self.train_numeral_labels is a list that is used to store all the labels of categories each of the quesions in the training data belongs to. It contains 6 labels for each of the data ABBR, ENTY etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  1 \t Question:  manner how did serfdom develop in and then leave russia ? \n",
      "\n",
      "label:  2 \t Question:  cremat what films featured the character popeye doyle ? \n",
      "\n",
      "label:  1 \t Question:  manner how can i find a list of celebrities ' real names ? \n",
      "\n",
      "label:  2 \t Question:  animal what fowl grabs the spotlight after the chinese year of the monkey ? \n",
      "\n",
      "label:  0 \t Question:  exp what is the full form of .com ? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "for i in range(5):\n",
    "    print('label: ',dm.train_numeral_labels[i],'\\t','Question: ',dm.train_str_questions[i],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.2**</span> \n",
    "**What is the purpose of `self.train_numeral_data`? Write your code to print out the first five questions in the numeric format with labels in the training set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "self.train_numeral_data initially is used to convert each text in each of the questions into a sequence of integers. It then pads all the sequences to the same length. Since padding='post', the 0's will get added after the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  29    8   19 3497 2219    5   16  433  814  990    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  32    2  815  619    1  148 1255 3498    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  29    8   37   35   67    6  484    4 1614   88  334  186    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  46    2 3499 3500    1 3501  156    1  485   68    4    1 1615    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [  81    2    3    1  486  300    4  396    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "print(dm.train_numeral_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.3**</span> \n",
    "**What is the purpose of two dictionaries: `self.word2idx` and `self.idx2word`? Write your code to print out the first five key-value pairs of those dictionaries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "self.word2idx is a dictionary which is used to store the words in train_set as keys and the index value of those text as its values, whereas self.idx2word is used to store all the index value of all the words as keys and the words are stored as corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In word2idx.....\n",
      "\n",
      "key:  the \t value:  1 \n",
      "\n",
      "key:  what \t value:  2 \n",
      "\n",
      "key:  is \t value:  3 \n",
      "\n",
      "key:  of \t value:  4 \n",
      "\n",
      "key:  in \t value:  5 \n",
      "\n",
      "In idx2word.....\n",
      "\n",
      "key:  1 \t value:  the \n",
      "\n",
      "key:  2 \t value:  what \n",
      "\n",
      "key:  3 \t value:  is \n",
      "\n",
      "key:  4 \t value:  of \n",
      "\n",
      "key:  5 \t value:  in \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "key_storage = []\n",
    "for keys in dm.word2idx:\n",
    "    key_storage.append(keys)\n",
    "del key_storage[5:len(key_storage)]\n",
    "print('In word2idx.....\\n')\n",
    "for keys in key_storage:\n",
    "    print('key: ',keys, '\\t', 'value: ',dm.word2idx[keys],'\\n')\n",
    "print('In idx2word.....\\n')\n",
    "for i in range(1,6):\n",
    "    print('key: ',i,'\\t','value: ',dm.idx2word[i],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.4**</span> \n",
    "**What is the purpose of `self.tf_train_set`? Write your code to print out the first five items of `self.tf_train_set`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "self.tf_train_set is used to convert the values of self.train_numeral_data and self.train_numeral_labels into a TensorSliceDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five elements of self.tf_train_set are....\n",
      "\n",
      "[(<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  29,    8,   19, 3497, 2219,    5,   16,  433,  814,  990,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  32,    2,  815,  619,    1,  148, 1255, 3498,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  29,    8,   37,   35,   67,    6,  484,    4, 1614,   88,  334,\n",
      "        186,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  46,    2, 3499, 3500,    1, 3501,  156,    1,  485,   68,    4,\n",
      "          1, 1615,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([ 81,   2,   3,   1, 486, 300,   4, 396,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)]\n",
      "\n",
      "Much cleaner output will be...\n",
      "\n",
      "([29 8 19 ... 0 0 0], 1)\n",
      "([32 2 815 ... 0 0 0], 2)\n",
      "([29 8 37 ... 0 0 0], 1)\n",
      "([46 2 3499 ... 0 0 0], 2)\n",
      "([81 2 3 ... 0 0 0], 0)\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "tensor_store = []\n",
    "for each in dm.tf_train_set:\n",
    "    tensor_store.append(each)\n",
    "del tensor_store[5:]\n",
    "print('The first five elements of self.tf_train_set are....\\n')\n",
    "print(tensor_store)\n",
    "print('\\nMuch cleaner output will be...\\n')\n",
    "for each in tensor_store:\n",
    "    tf.print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.5**</span> \n",
    "**What is the purpose of `self.tf_valid_set`? Write your code to print out the first five items of `self.tf_valid_set`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "self.tf_train_set is used to tune the hyperparameters/check generalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first five elements of self.tf_valid_set are....\n",
      "\n",
      "[(<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  38,   12,  279,    1,   33, 2178,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=3>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  27,    2, 6443,  584,   27,   55,    1, 6444,  158, 6445,    1,\n",
      "       6446,  158,   69, 6447,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  30,    2,  765,   30,    4, 6448,   63, 6449, 3299, 6450, 6451,\n",
      "         16, 6452,  106,  978,    5,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=4>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  74,    2,   74,  601,  685,   14,   92,   69, 6453,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>), (<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
      "array([  60,    2,    3,    6,  275, 2744, 2179,   70,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)]\n",
      "\n",
      "Much cleaner output will be...\n",
      "\n",
      "([38 12 279 ... 0 0 0], 3)\n",
      "([27 2 6443 ... 0 0 0], 4)\n",
      "([30 2 765 ... 0 0 0], 4)\n",
      "([74 2 74 ... 0 0 0], 2)\n",
      "([60 2 3 ... 0 0 0], 2)\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "tensor_store = []\n",
    "for each in dm.tf_valid_set:\n",
    "    tensor_store.append(each)\n",
    "del tensor_store[5:]\n",
    "print('The first five elements of self.tf_valid_set are....\\n')\n",
    "print(tensor_store)\n",
    "print('\\nMuch cleaner output will be...\\n')\n",
    "for each in tensor_store:\n",
    "    tf.print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Using Word2Vect to transform texts to vectors </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will be assessed on how to use a pretrained Word2Vect model for realizing a machine learning task. Basically, you will use this pretrained Word2Vect to transform the questions in the above dataset stored in the *data manager object dm* to numeric form for training a Support Vector Machine in sckit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.1**</span> \n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vect = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.2**</span> \n",
    "\n",
    "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        vector = model[word]\n",
    "    except: #word not in the vocabulary\n",
    "        vector = np.zeros(shape=(100,))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Text CNN for sequence modeling and neural embedding </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1**</span> \n",
    "\n",
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size \\times vocab\\_size \\times embed\\_size]$ as $h$.\n",
    "  - We feed $h$ to three Convd 1D layers, each of which has $state\\_size$ filters, padding=same, activation= relu, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size \\times output\\_size \\times state\\_size]$.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer on the top of $h$ for classification.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.data_manager.vocab_size +1, self.embed_size)(x)\n",
    "        h1 = tf.keras.layers.Conv1D(filters=self.state_size, padding='same', activation='relu', kernel_size=3)(h)#Insert your code here\n",
    "        h2 = tf.keras.layers.Conv1D(filters=self.state_size, padding='same', activation='relu', kernel_size=5)(h)#Insert your code here\n",
    "        h3 = tf.keras.layers.Conv1D(filters=self.state_size, padding='same', activation='relu', kernel_size=7)(h)#Insert your code here\n",
    "        max_pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        h1 = max_pool(h1)\n",
    "        h2 = max_pool(h2)\n",
    "        h3 = max_pool(h3)\n",
    "        h = tf.concat([h1,h2,h3], axis = 0)\n",
    "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h) \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: RNNs for sequence modeling and neural embedding </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4.1. One-directional RNNs for sequence modeling and neural embedding </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.1**</span> \n",
    "**In this part, you need to construct an RNN to learn from the dataset of interest. Basically, you are required first to construct the class UniRNN (Uni-directional RNN) with the following requirements:**\n",
    "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
    "- Attribute `cell_type (self.cell_type)`: can receive three values including `basic_rnn`, `gru`, and `lstm` which specifies the memory cells formed a hidden layer.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n",
    "**Note that when declaring an embedding layer for the network, you need to set *mask_zero=True* so that the padding zeros in the sentences will be masked and ignored. This helps to have variable length RNNs. For more detail, you can refer to this [link](https://www.tensorflow.org/guide/keras/masking_and_padding).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1 \n",
    "        \n",
    "    #return the correspoding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation)\n",
    "        elif cell_type== 'lstm':\n",
    "            return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation)\n",
    "        else:\n",
    "            return tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences, activation=activation)\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True)(x)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        return_sequences = True\n",
    "        for i in range(num_layers):\n",
    "            h =  self.get_layer(state_size=self.state_sizes[i], return_sequences = return_sequences, cell_type=self.cell_type)(h)\n",
    "            return_sequences = False\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "   \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.2**</span> \n",
    "**Run with basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 19s 374ms/step - loss: 0.5809 - accuracy: 0.8090 - val_loss: 0.5026 - val_accuracy: 0.8661\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 13s 242ms/step - loss: 0.1404 - accuracy: 0.9542 - val_loss: 0.1028 - val_accuracy: 0.9628\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 13s 255ms/step - loss: 0.1273 - accuracy: 0.9655 - val_loss: 0.2673 - val_accuracy: 0.9064\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 13s 249ms/step - loss: 0.0657 - accuracy: 0.9807 - val_loss: 0.4217 - val_accuracy: 0.8775\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 13s 256ms/step - loss: 0.0437 - accuracy: 0.9887 - val_loss: 0.0850 - val_accuracy: 0.9743\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 13s 251ms/step - loss: 0.0598 - accuracy: 0.9841 - val_loss: 0.0814 - val_accuracy: 0.9743\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 13s 250ms/step - loss: 0.0457 - accuracy: 0.9881 - val_loss: 0.0750 - val_accuracy: 0.9784\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 13s 244ms/step - loss: 0.0200 - accuracy: 0.9960 - val_loss: 0.1003 - val_accuracy: 0.9752\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 13s 247ms/step - loss: 0.0277 - accuracy: 0.9942 - val_loss: 0.1177 - val_accuracy: 0.9739\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 13s 246ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.1104 - val_accuracy: 0.9766\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 13s 246ms/step - loss: 0.0340 - accuracy: 0.9921 - val_loss: 0.1144 - val_accuracy: 0.9761\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 13s 246ms/step - loss: 4.1049e-04 - accuracy: 1.0000 - val_loss: 0.1360 - val_accuracy: 0.9766\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 13s 246ms/step - loss: 0.0405 - accuracy: 0.9930 - val_loss: 0.1124 - val_accuracy: 0.9771\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 13s 244ms/step - loss: 2.0105e-04 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9761\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 13s 247ms/step - loss: 0.0248 - accuracy: 0.9942 - val_loss: 0.1180 - val_accuracy: 0.9761\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 13s 248ms/step - loss: 2.4967e-04 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9706\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 13s 247ms/step - loss: 4.8154e-05 - accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9739\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 13s 244ms/step - loss: 8.6235e-06 - accuracy: 1.0000 - val_loss: 0.1781 - val_accuracy: 0.9757\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 13s 252ms/step - loss: 1.1109e-06 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.9739\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 13s 251ms/step - loss: 1.5798e-07 - accuracy: 1.0000 - val_loss: 0.2312 - val_accuracy: 0.9734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a6e35457c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type='basic_rnn',embed_size = 128, state_sizes = [128,128], data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.3**</span> \n",
    "**Run with GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 47s 898ms/step - loss: 0.8961 - accuracy: 0.6498 - val_loss: 0.2492 - val_accuracy: 0.9229\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 25s 488ms/step - loss: 0.1242 - accuracy: 0.9627 - val_loss: 0.1151 - val_accuracy: 0.9638\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 25s 487ms/step - loss: 0.0464 - accuracy: 0.9866 - val_loss: 0.1005 - val_accuracy: 0.9688\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 25s 489ms/step - loss: 0.0238 - accuracy: 0.9939 - val_loss: 0.1117 - val_accuracy: 0.9665\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.1841 - val_accuracy: 0.9633\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 25s 490ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.1082 - val_accuracy: 0.9725\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 26s 507ms/step - loss: 0.0229 - accuracy: 0.9969 - val_loss: 0.1189 - val_accuracy: 0.9706\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 27s 514ms/step - loss: 6.4886e-04 - accuracy: 0.9997 - val_loss: 0.1316 - val_accuracy: 0.9752\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 26s 500ms/step - loss: 2.2417e-04 - accuracy: 1.0000 - val_loss: 0.1613 - val_accuracy: 0.9716\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 26s 509ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.2377 - val_accuracy: 0.9638\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 27s 510ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.2065 - val_accuracy: 0.9661\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 27s 514ms/step - loss: 1.3166e-05 - accuracy: 1.0000 - val_loss: 0.1811 - val_accuracy: 0.9711\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 26s 498ms/step - loss: 4.5048e-06 - accuracy: 1.0000 - val_loss: 0.1868 - val_accuracy: 0.9725\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 26s 498ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.2304 - val_accuracy: 0.9688\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 26s 506ms/step - loss: 1.8344e-06 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9716\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 26s 493ms/step - loss: 6.4946e-07 - accuracy: 1.0000 - val_loss: 0.2024 - val_accuracy: 0.9739\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 26s 500ms/step - loss: 2.4413e-07 - accuracy: 1.0000 - val_loss: 0.2049 - val_accuracy: 0.9734\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 26s 502ms/step - loss: 1.0124e-07 - accuracy: 1.0000 - val_loss: 0.2119 - val_accuracy: 0.9734\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 26s 499ms/step - loss: 4.6316e-08 - accuracy: 1.0000 - val_loss: 0.2184 - val_accuracy: 0.9739\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 27s 515ms/step - loss: 3.1236e-08 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a6ed8eb4c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type='gru',embed_size = 128, state_sizes = [128,128], data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.4**</span> \n",
    "**Run with LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 46s 891ms/step - loss: 0.6968 - accuracy: 0.7659 - val_loss: 0.2035 - val_accuracy: 0.9427\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 21s 410ms/step - loss: 0.1246 - accuracy: 0.9639 - val_loss: 0.1031 - val_accuracy: 0.9674\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 0.0585 - accuracy: 0.9847 - val_loss: 0.0920 - val_accuracy: 0.9702\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.0380 - accuracy: 0.9893 - val_loss: 0.0895 - val_accuracy: 0.9725\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 0.0177 - accuracy: 0.9960 - val_loss: 0.0998 - val_accuracy: 0.9729\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.0156 - accuracy: 0.9963 - val_loss: 0.1110 - val_accuracy: 0.9748\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 20s 385ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.1984 - val_accuracy: 0.9688\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.1514 - val_accuracy: 0.9757\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.1657 - val_accuracy: 0.9752\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.1405 - val_accuracy: 0.9798\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 1.4993e-04 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9794\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.1732 - val_accuracy: 0.9789\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 1.8996e-05 - accuracy: 1.0000 - val_loss: 0.1769 - val_accuracy: 0.9794\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 6.0336e-06 - accuracy: 1.0000 - val_loss: 0.1926 - val_accuracy: 0.9794\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 0.0079 - accuracy: 0.9991 - val_loss: 0.2137 - val_accuracy: 0.9757\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 20s 385ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.2104 - val_accuracy: 0.9775\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 1.8050e-06 - accuracy: 1.0000 - val_loss: 0.2080 - val_accuracy: 0.9784\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 21s 394ms/step - loss: 1.0594e-06 - accuracy: 1.0000 - val_loss: 0.2131 - val_accuracy: 0.9789\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 4.4093e-07 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9775\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 21s 396ms/step - loss: 1.5798e-07 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a68fa26608>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(cell_type='lstm',embed_size = 128, state_sizes = [128,128], data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1.5**</span> \n",
    "**Give your own comments about the performance of three memory cells for the dataset of interest as well as what happening during the training process of each cell. Note that there are not right or wrong comments and your comments rely on the status of your training. In addition, some comments and hypothesized assessments of what and why are occurring are useful to obtain the highest score for this question.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Out of the three memory cells, basic rnn tends to produce the lowest accuracy and lstm and gru giving the highest accuracies. In Basic RNN, the problem of Gradient vanishing/explosion can occur. In order to address this problem, tanh function is used, which is used to scale the output to (-1,1) at each time step. In GRU, the update gate decides how much each unit updates its state, the reset gate decides which parts of the state will be used to compute the next target state. LSTM consists of two states, long term state and the short term state. The forget gate in LSTM controls how much of the previous long term memory elements should be forgotten, the output gate controls how much of the long term memory elements should be carried on to the next time slice and the input gate controls how much to absorb more from the current information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4.2. Bi-directional RNNs for sequence modeling and neural embedding </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.1**</span> \n",
    "**In what follow, you will investigate BiRNN. The task is similar to Part 4.1 but you need to write the code for an BiRNN. Note that the function *get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh')* has to return the hidden layer with bidirectional memory cells (e.g., Basic RNN, GRU, and LSTM cells).**\n",
    "\n",
    "**Complete the code of the class *BiRNN*. Note that for the embedding layer you need to set *mask_zero=True*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation))\n",
    "        elif cell_type== 'lstm':\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation))\n",
    "        else:\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences, activation=activation))\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True)(x)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        return_sequences = True\n",
    "        for i in range(num_layers):\n",
    "            h = self.get_layer(state_size=self.state_sizes[i], return_sequences = return_sequences, cell_type=self.cell_type)(h)\n",
    "            return_sequences = False\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.2**</span> \n",
    "**Run BiRNN for basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 26s 492ms/step - loss: 0.3239 - accuracy: 0.8897 - val_loss: 0.1113 - val_accuracy: 0.9606\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 22s 419ms/step - loss: 0.0667 - accuracy: 0.9774 - val_loss: 0.0877 - val_accuracy: 0.9633\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 22s 416ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.1215 - val_accuracy: 0.9642\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 22s 418ms/step - loss: 0.0058 - accuracy: 0.9979 - val_loss: 0.1025 - val_accuracy: 0.9693\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 22s 431ms/step - loss: 2.5191e-04 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9729\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 22s 422ms/step - loss: 2.2701e-05 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 0.9725\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 22s 427ms/step - loss: 2.5751e-06 - accuracy: 1.0000 - val_loss: 0.1360 - val_accuracy: 0.9725\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 26s 502ms/step - loss: 2.6099e-07 - accuracy: 1.0000 - val_loss: 0.1532 - val_accuracy: 0.9720\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 24s 454ms/step - loss: 4.7871e-08 - accuracy: 1.0000 - val_loss: 0.1620 - val_accuracy: 0.9716\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 23s 444ms/step - loss: 1.9564e-08 - accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9720\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 23s 442ms/step - loss: 1.1394e-08 - accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9716\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 23s 439ms/step - loss: 7.6680e-09 - accuracy: 1.0000 - val_loss: 0.1663 - val_accuracy: 0.9720\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 23s 447ms/step - loss: 5.8047e-09 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 0.9720\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 23s 441ms/step - loss: 4.5506e-09 - accuracy: 1.0000 - val_loss: 0.1649 - val_accuracy: 0.9720\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 23s 437ms/step - loss: 3.6548e-09 - accuracy: 1.0000 - val_loss: 0.1661 - val_accuracy: 0.9725\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 23s 436ms/step - loss: 2.9740e-09 - accuracy: 1.0000 - val_loss: 0.1655 - val_accuracy: 0.9725\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 23s 435ms/step - loss: 2.2932e-09 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 0.9725\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 24s 454ms/step - loss: 2.0424e-09 - accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9729\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 23s 441ms/step - loss: 1.6124e-09 - accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9729\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 23s 434ms/step - loss: 1.2541e-09 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a6a2d3d548>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type='basic_rnn', embed_size=128, state_sizes=[128,128], data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.3**</span> \n",
    "**Run BiRNN for GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 63s 1s/step - loss: 0.5641 - accuracy: 0.8023 - val_loss: 0.1691 - val_accuracy: 0.9445\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 44s 853ms/step - loss: 0.0786 - accuracy: 0.9743 - val_loss: 0.1232 - val_accuracy: 0.9647\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 44s 851ms/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.1275 - val_accuracy: 0.9656\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 44s 853ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 0.1463 - val_accuracy: 0.9683\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 44s 850ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.1562 - val_accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 45s 866ms/step - loss: 3.1494e-04 - accuracy: 1.0000 - val_loss: 0.1353 - val_accuracy: 0.9720\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 44s 840ms/step - loss: 1.3594e-05 - accuracy: 1.0000 - val_loss: 0.1661 - val_accuracy: 0.9706\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 44s 847ms/step - loss: 1.6123e-06 - accuracy: 1.0000 - val_loss: 0.2040 - val_accuracy: 0.9697\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 44s 838ms/step - loss: 2.4090e-07 - accuracy: 1.0000 - val_loss: 0.2329 - val_accuracy: 0.9693\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 44s 840ms/step - loss: 6.5495e-08 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 0.9688\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 46s 878ms/step - loss: 3.3216e-08 - accuracy: 1.0000 - val_loss: 0.2522 - val_accuracy: 0.9688\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 44s 844ms/step - loss: 2.1105e-08 - accuracy: 1.0000 - val_loss: 0.2560 - val_accuracy: 0.9697\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 44s 839ms/step - loss: 1.5623e-08 - accuracy: 1.0000 - val_loss: 0.2590 - val_accuracy: 0.9697\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 44s 844ms/step - loss: 1.2362e-08 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9693\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 44s 837ms/step - loss: 1.0212e-08 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9693\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 46s 876ms/step - loss: 8.5996e-09 - accuracy: 1.0000 - val_loss: 0.2657 - val_accuracy: 0.9693\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 44s 853ms/step - loss: 7.3813e-09 - accuracy: 1.0000 - val_loss: 0.2673 - val_accuracy: 0.9693\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 44s 839ms/step - loss: 6.4855e-09 - accuracy: 1.0000 - val_loss: 0.2687 - val_accuracy: 0.9693\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 44s 845ms/step - loss: 5.6972e-09 - accuracy: 1.0000 - val_loss: 0.2702 - val_accuracy: 0.9693\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 44s 838ms/step - loss: 4.9089e-09 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a7096064c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type='gru', embed_size=128, state_sizes=[128,128], data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.4**</span> \n",
    "**Run BiRNN for LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 62s 1s/step - loss: 0.6440 - accuracy: 0.7861 - val_loss: 0.1986 - val_accuracy: 0.9399\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 41s 795ms/step - loss: 0.0931 - accuracy: 0.9731 - val_loss: 0.1121 - val_accuracy: 0.9683\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 42s 800ms/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.1173 - val_accuracy: 0.9743\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 42s 804ms/step - loss: 0.0167 - accuracy: 0.9960 - val_loss: 0.2896 - val_accuracy: 0.9541\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 42s 811ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.1195 - val_accuracy: 0.9752\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 43s 819ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.1500 - val_accuracy: 0.9729\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 41s 790ms/step - loss: 5.9010e-04 - accuracy: 0.9997 - val_loss: 0.1458 - val_accuracy: 0.9739\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 41s 791ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 0.1383 - val_accuracy: 0.9752\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 41s 790ms/step - loss: 2.1566e-05 - accuracy: 1.0000 - val_loss: 0.1341 - val_accuracy: 0.9761\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 41s 795ms/step - loss: 8.5599e-06 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9766\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 44s 851ms/step - loss: 1.9105e-06 - accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9780\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 42s 800ms/step - loss: 3.4461e-07 - accuracy: 1.0000 - val_loss: 0.1880 - val_accuracy: 0.9775\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 42s 800ms/step - loss: 1.0362e-07 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9771\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 42s 806ms/step - loss: 4.9658e-08 - accuracy: 1.0000 - val_loss: 0.2104 - val_accuracy: 0.9766\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 42s 802ms/step - loss: 3.2177e-08 - accuracy: 1.0000 - val_loss: 0.2124 - val_accuracy: 0.9771\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 43s 822ms/step - loss: 2.4151e-08 - accuracy: 1.0000 - val_loss: 0.2148 - val_accuracy: 0.9766\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 42s 814ms/step - loss: 1.8561e-08 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.9766\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 42s 799ms/step - loss: 1.4942e-08 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9757\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 41s 797ms/step - loss: 1.2398e-08 - accuracy: 1.0000 - val_loss: 0.2191 - val_accuracy: 0.9761\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 43s 825ms/step - loss: 1.0140e-08 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a741f46e88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(cell_type='lstm', embed_size=128, state_sizes=[128,128], data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2.5**</span> \n",
    "\n",
    "**Give your own comments about the performance of three memory cells for the dataset of interest as well as comparing BiRNN to UniRNN in Part 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans:**\n",
    "Out of the three variations in the memory cells, gru tends to perform the best followed by lstm and then basic rnn. From observing their behaviour between UniRNN and BiRNN, one can observe that the performance in BiRNN is better for gru, whereas there is a small variation for the accuracy between UniRNN and BiRNN for lstm, while the accuracy of basic rnn remains same for both the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<div style=\"text-align: center\"> <span style=\"color:black\">END OF ASSIGNMENT</span> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
